1. Wczytaj dane, obejrzyj je i podsumuj w dwóch-trzech zdaniach. 
```{r}
people <- read.csv("people.tab", header=TRUE, sep="\t")
attach(people)
View(people)
```


pozostałe zmienne są zmiennymi objaśniającymi (niezależnymi):
```age, weight, height, gender, married, number_of_kids, pet```


### Zadanie1.
Ile jest obserwacji, ile zmiennych ilościowych, a ile jakościowych? 
#### Zmienne jakościowe:
married, gender, pet 
#### Zmienne ilościowe:
* age, weight, height, number_of_kids, expenses
### Obserwacji:
500.

```{r}
length(people$age) # obserwacja ilość
```
### Zmienne objaśniająće: 



Czy są zależności w zmiennych objaśniających (policz i podaj VIF)? 

VIF - czynnik infalcji wariancji

```{r}
#install.packages("car")

library(car)
model_rl <- lm(expenses ~ ., data=people)
vif(model_rl)
```

VIF dla każdego z predyktorów jest w okolicach 1 lub 2, wówczas nie mamy przesłanek by twierdzić dużej współliniowości predyktór. 


Czy występują jakieś braki danych?
```{r}
which(is.na(people))
```
To oznacza, że nie mamy braków w danych. 

UWAGA: zmienną ```expenses``` z ujemnymi wagami interpretuje jako przychód


2. Podsumuj dane przynajmniej trzema różnymi wykresami. Należy przygotować:

Wykres pokazująćy jak zmienne objaśniające zależa od siebie, oraz jak zmienna objaśniana ```expenses``` zależy od wszystkich
```{r}
my_cols <- c("blue", "yellow")  
pairs(people[, c(1,2,3,4,5,6,7,8)], pch = 19,  cex = 0.7, col = my_cols[], lower.panel = NULL)
```

```{r}
 boxplot(number_of_kids~pet, data=people, main="Kids love pets",
   xlab="Pets", ylab="Numer of kids")
```
```{r}
counts <- table(people$married, people$pet)
barplot(counts, main="Married Distribution ",
   xlab="Number of pet", legend = rownames(counts),  beside=TRUE)
```


Dodatkowo: 
```{r}
library(ggplot2)
ggplot(people) + geom_histogram(aes(x=age), binwidth=5) + theme_minimal()
```

### Zadanie2 


####  Róznica między średnią wartością wybranej zmiennej:

Wykorzystuje test t-Studenta dla populacji o różnych licznościach, ale równych wariancjach (Wykład 2). Zakładamy poziom istotności $\alpha = 0.05$ 
```{r}
shapiro.test(people$weight)
```
p-wartość > 0.05 daję nam, że dane pochodzą z rozkładu normalnego.  

Zakładamy, że średnia zarobków $\mu_1$ dla kobiet jest równa średniej zarobków dla mężczyzn $\mu_2$:
$$H_0: \mu_1 = \mu_2$$
Przeciwko hipotezie alternatwynej: że jest różna
$$H_1: \mu_1 \neq \mu_2$$


```{r}
x = people$weight[people$gender == "man"] # filtruje dane
y = people$weight[people$gender == "woman"] #

t.test(x, y, var.equal = TRUE) # wykonuje test
```
Decyzja:
p-wartość = $0.1516 > \alpha = 0.05$, wówczas: wybieram hipotezę alternatywną, że różnica w średnich nie jest równa 0, tzn średnie są różne. 


#### Niezależności między dwoma zmiennymi ilościowymi:
##### Test Spearmana:
Dana jest próba losowa postaci $(X_1, Y_1), \ldots (X_n,Y_n)$, gdzie X to ```expenses``` a Y to ```age``` 

```{r}
  X = people$expenses
  Y = people$age
```
Formuje hipotezę:
$$H_0: \text{X i Y są niezależne}$$
przeciw:
$$H_1: \text{istnieje jakikolwiek rodzaj zależności między X i Y}$$

Wyliczamy statystykę testu Pearsona.  
```{r}
cor(X,Y,method="pearson")
```

```{r}
cor.test(X, Y,method="pearson")
```
Nasz współcznnik jest na poziomie 0.8., wówczas przyjmujemy hipotezę alternatywną, że nasze dane są zależne. 


#### Jedną dot. niezależności między dwoma zmiennymi jakościowymi

W tym celu wykorzystam Chi-square pearsona.
$$H_0: \text{zmienne X i Y są niezależne} $$
przeciwko:
$$
  H_1: \text{zmienne X i Y są zależne}  
$$
Wynkonuje test na poziomie istotności $\alpha = 0.05$

Buduję macierz kontygnencji:

```{r}
  confusion_matrix<- table(people$gender, people$married)
```

```{r}
head(confusion_matrix)
```

```{r}
chisq <- chisq.test(confusion_matrix)
```

```{r}
chisq
```
 $p-value > \alpha$, więc nie ma zależności pomiędzy grupami.



#### Jedna dot. rozkładu zmiennej (np. "zmienna A ma rozkład wykładniczy z parametrem 10")

W tym celu przeprowadzę test: Kołomogorowa dla zmiennej ```expences```:

I wprowadzam hipotezę, na poziomie istotności $\alpha = 0.05$
$$H_0: \text{zmienna A ma rozkład wykładniczy z parametrem 10}$$
przeciw:
$$H_1 \text{ zmienna A nie ma rozkładu wykładniczego z parametrem 10}$$

```{r}
  ks.test(height, "pexp", 3)
```
Statystyka testowa $D = 0.802$, wówczas nasza p-wartość jest mniejsza niż poziom istotności. 
Uważam, że nie ma znaczącej różnicy między rozkładami, nie ma podstaw do odrzucenia hipotezy $H_0$.


### Zadanie3
3.Podaj przedziały ufności dla wartości średniej i wariancji dla zmiennych wiek i wzrost.
Jeżeli w celu wyliczenia przedziału ufności musisz poczynić jakieś założenia(np.założyć że zmienna pochodzi z rozkładu normalnego), zaznacz to i skomentuj czy wydaje Ci się to w danym przypadku uprawnione.
Opisz wszelkie dodatkowe operacje, jakie zostały wykonane przed testem(takie jak usunięcie obserwacji odstających).
Przedyskutuj, dla której ze zmiennych oczekujesz prawidłowych wyników. 


W celu sprawdzenia normalności danych wykonuje test Test Shapiro-Wilka:
$$
     H_0 : \text{Wzrost pochodzi z populacji o rozkładzie normalnym}
$$
przeciw alternatywnej: 
$$
    H_1: \text{Wzrost nie pochodzi z populacji o rozkładzie normalnym}
$$

```{r}
shapiro.test(people$height)
```
$\texttt{p-wartość} > 0.05$
wówczas rozkład naszych danych nie znacząco różni się od rozkładu normalego. Więc dane pochodzą właśnie z takiego rozkładu. 

I analogicznie dla zmiennej ```age```

```{r}
shapiro.test(people$age)
```
Widzmy, że zmienna dla zmiennej ```age``` $p < \alpha$ wówczas odrzucamy hipotezę zerowa. 

Pokazuje to również wykres kwantylowy
```{r}
library(ggplot2)

qqplot_age <- ggplot(people) + stat_qq(aes(sample=age)) + stat_qq_line(aes(sample=age)) + theme_minimal() + ggtitle('Wykres kwantylowy wieku') + xlab('Kwantyl teoretyczny') + ylab('Kwantyl obserwowany')
# Usuwam tylko nie wygodne dane, 
qqplot_age
```



Postanowiłem usnąc tylko obserwacje, które nie są naturalne dla zmiennej ```height```. ***Stosując metodę 1.5 IQR, usunąłbym za dużo znaczących zmiennych. ***

```{r}
library(ggplot2)
qqplot_height <- ggplot(people) + stat_qq(aes(sample=height)) + stat_qq_line(aes(sample=height)) + theme_minimal() + ggtitle('Wykres kwantylowy wysokości') + xlab('Theretical Quantiles') + ylab('Sample Quantiles')
qqplot_height
# Usuwam wysokości odstające mocno(mogą sugerować błędne dane)
people <- people[-c(241, 356, 419, 458, 194, 51, 64, 161, 364, 413, 256, 207, 39, 199, 215, 362, 247, 467), ]

qqplot_age <- ggplot(people) + stat_qq(aes(sample=height)) + stat_qq_line(aes(sample=height)) + theme_minimal() + ggtitle('Wykres kwantylowy wysokości bez odstających') + xlab('Theretical Quantiles') + ylab('Sample Quantiles')
qqplot_age
summary(people$height)
```

Widzimy z wykresów kwantowlowych wysokości, że założenie o rozkładu próbki jako rozkładu normalności jest spełnione. 


Przy założeniu, że zmienna ```height``` ma rozkład normalny, ustalam próg ufności $\alpha = 0.05$.
### Przedział ufności dla średniej
```{r}
alpha <- 0.05
X = people$height
n <- nrow(people)
q <- qt(1-alpha/2, n-1)
height_mean = mean(X)
height_sd = sd(X)
student.interval <- c(height_mean - q*height_sd/sqrt(n-1), height_mean + q*height_sd/sqrt(n-1))
print(student.interval)
```
```{r}
height_mean
height_sd
```
Dla 95 na 100 powtórzeń obliczony przedział będzie zawierał prawdziwą nieznaną wartość średnią dla zmiennej ```height``` 

### Przedział ufności dla odchylenia standardowego 

na poziomie istotności $\alpha = 0.05$
```{r}
sqrt(height_sd*length(X)/qchisq(c(1-.025,.025), length(X)-1))
```


### Zadanie5

Oszacuj model regresji linowej, przyjmując za zmienną zależną (y) wydatki domowe (expenses) a zmienne niezależne (x) wybierając spośród pozostałych zmiennych. Rozważ, czy konieczne są transformacje zmiennych lub zmiennej objaśnianej.

Podaj RSS, $R^2$ , p-wartości i oszacowania współczynników i wybierz właściwe zmienne objaśniające, które najlepiej tłumaczą ```expenses```. Sprawdź czy w wybranym przez Ciebie modelu spełnione są założenia modelu liniowego i przedstaw na wykresach diagnostycznych: wykresie zależności reszt od zmiennej objaśnianej,na wykresie reszt studentyzowanych i na wykresie dźwigni i przedyskutuj, czy są spełnione.

Mój pierwszy krok, to zbudowanie modelu z wszystkimi predyktorami, a więc:
```expenses ~ .```


Następnie chce wybrać tylko istotne predyktory. 



Z wykładu wykorzystuje podstawowe techniki wyszukiwania
```{r}
library(MASS)
# Fit the full model 
full.model <- lm(expenses ~., data = people)
summ_full <- summary(full.model)
summ_full
```
```{r}
RSS <- sum(resid(full.model)^2)
RSS
r2 <- summ_full$r.squared
r2
```

### Stepwise regression model- podejście z wykładu
```{r}
# Stepwise regression model
step.model <- stepAIC(full.model, direction = "both", 
                      trace = FALSE, x = TRUE)
summary_step = summary(step.model)
```

#### Sprawdzenie, czy w moim modelu są spełnione założenia modelu linowego. 

Resztowa suma kwadratów

```{r}
RSS <- sum(resid(step.model)^2)
RSS
r2 <- summary_step$r.squared
r2
```
P-wartość: 
p-value: < 2.2e-16





#### Wykres wartości resztowych: 
```{r}
par(mfrow=c(1,1))
plot(step.model, pch=23, bg='pink', cex=0.7, which= 1)
```
widzimy, że linowość jest naruszona Wydaje się, że jest to relacja kwadratowa. 
Nie zauważamy wykresy "lejka", który sugeruję zmienność wariancji. Można przyjać założenie modelu, że każdy błąd ma tą samą wariancje.

Na wykresie widzimy również oberwacje odstające: ```262```, ```188```, ```409```. Usunięcie takich danych zmieni miarę jakości dopasowania. 


#### Wykres dźwigni: 

Widzmy obserwacje o wysokiej dźwigni 409, 205, 188, ich usunięcie spowoduje duże zmiany w przebiegu prostej regresji:
```{r}
par(mfrow=c(1,1))
plot(step.model, pch=20 ,bg='pink', cex=0.5, which=5)
```


```{r}
plot(step.model, pch=23, bg='pink', cex=0.7, which= 2)
```

Odkrywanie obserwacji dźwigniowych 

```{r}
eruption.res = resid(step.model) 
```
### dodatkowa diagnostyka:

```{r}
library(gvlma)
gvmodel <- gvlma(step.model)
summary(gvmodel) 
```
Widać, że akceptujemy ```Heteroscedasticity```
```{r}
 vif(step.model) # variance inflation factors 
 sqrt(vif(step.model)) > 2 # nie mamy korelacji predyktorów
```

